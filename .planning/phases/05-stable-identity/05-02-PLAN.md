---
phase: 05-stable-identity
plan: 02
type: execute
wave: 1
depends_on: []
files_modified: [src/graph/mod.rs, src/graph/db_compat.rs]
autonomous: true
user_setup: []

must_haves:
  truths:
    - "execution_log table exists in SQLite database"
    - "CodeGraph::open() initializes ExecutionLog schema"
    - "MAGELLAN_SCHEMA_VERSION incremented to 2"
    - "Existing databases can be opened (migration via Option fields)"
  artifacts:
    - path: "src/graph/execution_log.rs"
      provides: "ExecutionLog module following ChunkStore pattern"
      exports: ["ExecutionLog", "start_execution", "finish_execution"]
    - path: "src/graph/mod.rs"
      provides: "ExecutionLog initialization in CodeGraph"
      contains: "execution_log"
    - path: "src/graph/db_compat.rs"
      provides: "Schema version increment"
      contains: "MAGELLAN_SCHEMA_VERSION: i64 = 2"
  key_links:
    - from: "src/graph/mod.rs"
      to: "src/graph/execution_log.rs"
      via: "ExecutionLog::new()"
      pattern: "ExecutionLog::new"
    - from: "src/graph/mod.rs"
      to: "src/graph/db_compat.rs"
      via: "MAGELLAN_SCHEMA_VERSION check"
      pattern: "ensure_magellan_meta"

---

<objective>
Create ExecutionLog module for tracking every Magellan run

Purpose: Enable users to correlate CLI outputs and database records with per-run execution_id, satisfying DB-03 requirement.
Output: ExecutionLog module with SQLite table, CodeGraph integration, schema version bump
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/05-stable-identity/05-RESEARCH.md

@src/generation/mod.rs
@src/graph/mod.rs
@src/graph/db_compat.rs
@src/output/command.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create execution_log.rs module with ExecutionLog struct</name>
  <files>src/graph/execution_log.rs</files>
  <action>
    Create new src/graph/execution_log.rs module following ChunkStore pattern:

    ```rust
    //! Execution log for tracking Magellan runs
    //!
    //! Records every CLI command execution with execution_id, timestamps,
    //! arguments, and outcome. Provides audit trail for correlating outputs.

    use anyhow::Result;
    use rusqlite::{params, OptionalExtension};
    use std::path::Path;

    /// Execution log entry
    #[derive(Debug, Clone)]
    pub struct ExecutionRecord {
        pub id: i64,
        pub execution_id: String,
        pub tool_version: String,
        pub args: String,  // JSON array
        pub root: Option<String>,
        pub db_path: String,
        pub started_at: i64,
        pub finished_at: Option<i64>,
        pub duration_ms: Option<i64>,
        pub outcome: String,  // "success", "error", "partial"
        pub error_message: Option<String>,
        pub files_indexed: i64,
        pub symbols_indexed: i64,
        pub references_indexed: i64,
    }

    /// Execution log storage
    ///
    /// Uses separate rusqlite connection to same database file.
    /// Follows ChunkStore pattern for side-table management.
    pub struct ExecutionLog {
        db_path: std::path::PathBuf,
    }

    impl ExecutionLog {
        pub fn new(db_path: &Path) -> Self {
            Self { db_path: db_path.to_path_buf() }
        }

        pub fn connect(&self) -> Result<rusqlite::Connection, rusqlite::Error> {
            rusqlite::Connection::open(&self.db_path)
        }

        pub fn ensure_schema(&self) -> Result<()> {
            let conn = self.connect()?;

            conn.execute(
                "CREATE TABLE IF NOT EXISTS execution_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    execution_id TEXT NOT NULL UNIQUE,
                    tool_version TEXT NOT NULL,
                    args TEXT NOT NULL,
                    root TEXT,
                    db_path TEXT NOT NULL,
                    started_at INTEGER NOT NULL,
                    finished_at INTEGER,
                    duration_ms INTEGER,
                    outcome TEXT NOT NULL,
                    error_message TEXT,
                    files_indexed INTEGER DEFAULT 0,
                    symbols_indexed INTEGER DEFAULT 0,
                    references_indexed INTEGER DEFAULT 0
                )",
                [],
            ).map_err(|e| anyhow::anyhow!("Failed to create execution_log table: {}", e))?;

            // Indexes
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_execution_log_started_at
                    ON execution_log(started_at DESC)",
                [],
            ).map_err(|e| anyhow::anyhow!("Failed to create started_at index: {}", e))?;

            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_execution_log_execution_id
                    ON execution_log(execution_id)",
                [],
            ).map_err(|e| anyhow::anyhow!("Failed to create execution_id index: {}", e))?;

            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_execution_log_outcome
                    ON execution_log(outcome)",
                [],
            ).map_err(|e| anyhow::anyhow!("Failed to create outcome index: {}", e))?;

            Ok(())
        }

        pub fn start_execution(
            &self,
            execution_id: &str,
            tool_version: &str,
            args: &[String],
            root: Option<&str>,
            db_path: &str,
        ) -> Result<i64> {
            let conn = self.connect()?;
            let args_json = serde_json::to_string(args)?;
            let started_at = std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs() as i64;

            conn.execute(
                "INSERT INTO execution_log
                    (execution_id, tool_version, args, root, db_path, started_at, outcome)
                    VALUES (?1, ?2, ?3, ?4, ?5, ?6, 'running')",
                params![execution_id, tool_version, args_json, root, db_path, started_at],
            ).map_err(|e| anyhow::anyhow!("Failed to insert execution log: {}", e))?;

            Ok(conn.last_insert_rowid())
        }

        pub fn finish_execution(
            &self,
            execution_id: &str,
            outcome: &str,
            error_message: Option<&str>,
            files_indexed: usize,
            symbols_indexed: usize,
            references_indexed: usize,
        ) -> Result<()> {
            let conn = self.connect()?;
            let finished_at = std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_secs() as i64;

            // Get started_at to compute duration
            let started_at: i64 = conn.query_row(
                "SELECT started_at FROM execution_log WHERE execution_id = ?1",
                params![execution_id],
                |row| row.get(0),
            ).optional()?.unwrap_or(finished_at);

            let duration_ms = (finished_at - started_at) * 1000;

            conn.execute(
                "UPDATE execution_log
                    SET finished_at = ?1, outcome = ?2, error_message = ?3,
                        duration_ms = ?4, files_indexed = ?5, symbols_indexed = ?6,
                        references_indexed = ?7
                    WHERE execution_id = ?8",
                params![
                    finished_at, outcome, error_message, duration_ms,
                    files_indexed as i64, symbols_indexed as i64, references_indexed as i64,
                    execution_id,
                ],
            ).map_err(|e| anyhow::anyhow!("Failed to update execution log: {}", e))?;

            Ok(())
        }
    }
    ```

    Follow ChunkStore pattern exactly:
    - new() returns struct with db_path
    - connect() opens rusqlite connection
    - ensure_schema() creates table + indexes
    - Use anyhow::Error for error messages
  </action>
  <verify>cargo check succeeds on new module</verify>
  <done>execution_log.rs module created with ExecutionLog struct and methods</done>
</task>

<task type="auto">
  <name>Task 2: Add execution_log module to graph/mod.rs</name>
  <files>src/graph/mod.rs</files>
  <action>
    Add execution_log module to src/graph/mod.rs:
    1. Add `mod execution_log;` at top of file after other modules
    2. Add `use crate::graph::execution_log::ExecutionLog;` after existing use statements
    3. Add `execution_log: ExecutionLog,` field to CodeGraph struct after chunks field
    4. Initialize in CodeGraph::open():
       - After chunks.ensure_schema()?
       - Add: `let execution_log = ExecutionLog::new(&db_path_buf);`
       - Add: `execution_log.ensure_schema()?;`
    5. Add execution_log to Self construction: execution_log,
  </action>
  <verify>cargo check succeeds</verify>
  <done>execution_log module added to graph, initialized in CodeGraph::open</done>
</task>

<task type="auto">
  <name>Task 3: Increment MAGELLAN_SCHEMA_VERSION to 2</name>
  <files>src/graph/db_compat.rs</files>
  <action>
    Update MAGELLAN_SCHEMA_VERSION in src/graph/db_compat.rs:
    - Change `pub const MAGELLAN_SCHEMA_VERSION: i64 = 1;` to `= 2;`
    - Update comment to "Phase 5: added execution_log table"
    - This will trigger compatibility check for existing databases

    Note: Existing databases will fail compat check. This is expected for Phase 5.
    Users can delete their DB or we can add migration in future.
  </action>
  <verify>cargo check succeeds</verify>
  <done>MAGELLAN_SCHEMA_VERSION = 2, references execution_log table</done>
</task>

<task type="auto">
  <name>Task 4: Add ExecutionLog tests</name>
  <files>src/graph/execution_log.rs</files>
  <action>
    Add #[cfg(test)] module tests to execution_log.rs:
    - test_execution_log_schema(): ensure_schema creates table
    - test_execution_log_insert_and_update(): start + finish execution
    - test_execution_log_duplicate_id(): UNIQUE constraint on execution_id
    - test_execution_outcome_values(): success/error/partial outcomes

    Use tempfile::tempdir() for test databases.
    Follow existing test patterns from generation/mod.rs
  </action>
  <verify>cargo test --lib graph::execution_log passes</verify>
  <done>All ExecutionLog tests pass</done>
</task>

</tasks>

<verification>
1. cargo check passes without errors
2. cargo test --lib includes passing execution_log tests
3. execution_log table schema verified in SQLite
4. CodeGraph::open() calls ExecutionLog::ensure_schema()
5. MAGELLAN_SCHEMA_VERSION = 2
</verification>

<success_criteria>
1. ExecutionLog module exists following ChunkStore pattern
2. execution_log table created with all required columns and indexes
3. CodeGraph initializes ExecutionLog on open
4. Schema version incremented to 2
5. All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-stable-identity/05-02-SUMMARY.md`
</output>
