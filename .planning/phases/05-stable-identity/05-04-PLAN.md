---
phase: 05-stable-identity
plan: 04
type: execute
wave: 2
depends_on: [05-02]
files_modified: [src/main.rs, src/query_cmd.rs, src/find_cmd.rs, src/refs_cmd.rs, src/files_cmd.rs, src/status_cmd.rs, src/export_cmd.rs, src/verify_cmd.rs, src/get_cmd.rs, src/watch_cmd.rs]
autonomous: true
user_setup: []

must_haves:
  truths:
    - "execution_id is generated once per CLI invocation"
    - "ExecutionLog::start_execution called at command start"
    - "ExecutionLog::finish_execution called at command completion"
    - "execution_id appears in all JSON responses"
    - "Errors are recorded with error_message in execution_log"
  artifacts:
    - path: "src/main.rs"
      provides: "execution_id generation and command wrapping"
      contains: "generate_execution_id"
    - path: "src/graph/mod.rs"
      provides: "CodeGraph exposes ExecutionLog"
      exports: ["execution_log"]
  key_links:
    - from: "src/main.rs"
      to: "src/graph/mod.rs"
      via: "CodeGraph::execution_log()"
      pattern: "execution_log"
    - from: "src/main.rs"
      to: "src/output/command.rs"
      via: "generate_execution_id()"
      pattern: "generate_execution_id"

---

<objective>
Wire up execution tracking in all command handlers

Purpose: Record every CLI command execution in execution_log table, satisfying ID-04 and DB-03 requirements.
Output: All commands generate execution_id, record start/finish, include execution_id in JSON outputs
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/05-stable-identity/05-RESEARCH.md

@.planning/phases/05-stable-identity/05-02-SUMMARY.md

@src/main.rs
@src/graph/mod.rs
@src/output/command.rs
@src/indexer.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Expose ExecutionLog from CodeGraph</name>
  <files>src/graph/mod.rs</files>
  <action>
    Add public method to CodeGraph to access ExecutionLog:
    ```rust
    /// Get the execution log for recording command execution
    pub fn execution_log(&self) -> &ExecutionLog {
        &self.execution_log
    }
    ```

    Add after count_chunks() method.
    This allows command handlers to record execution start/finish.
  </action>
  <verify>cargo check succeeds</verify>
  <done>CodeGraph exposes execution_log via public method</done>
</task>

<task type="auto">
  <name>Task 2: Add execution tracking helper to main.rs</name>
  <files>src/main.rs</files>
  <action>
    Add helper struct and function for execution tracking:

    ```rust
    /// Execution tracking wrapper
    ///
    /// Records execution start/finish in execution_log.
    /// Handles both success and error outcomes.
    struct ExecutionTracker {
        exec_id: String,
        tool_version: String,
        args: Vec<String>,
        root: Option<String>,
        db_path: String,
        outcome: String,
        error_message: Option<String>,
        files_indexed: usize,
        symbols_indexed: usize,
        references_indexed: usize,
    }

    impl ExecutionTracker {
        fn new(args: Vec<String>, root: Option<String>, db_path: String) -> Self {
            Self {
                exec_id: generate_execution_id(),
                tool_version: env!("CARGO_PKG_VERSION").to_string(),
                args,
                root,
                db_path,
                outcome: "success".to_string(),
                error_message: None,
                files_indexed: 0,
                symbols_indexed: 0,
                references_indexed: 0,
            }
        }

        fn start(&self, graph: &CodeGraph) -> Result<()> {
            graph.execution_log().start_execution(
                &self.exec_id,
                &self.tool_version,
                &self.args,
                self.root.as_deref(),
                &self.db_path,
            )
        }

        fn finish(&self, graph: &CodeGraph) -> Result<()> {
            graph.execution_log().finish_execution(
                &self.exec_id,
                &self.outcome,
                self.error_message.as_deref(),
                self.files_indexed,
                self.symbols_indexed,
                self.references_indexed,
            )
        }

        fn set_error(&mut self, msg: String) {
            self.outcome = "error".to_string();
            self.error_message = Some(msg);
        }

        fn set_counts(&mut self, files: usize, symbols: usize, references: usize) {
            self.files_indexed = files;
            self.symbols_indexed = symbols;
            self.references_indexed = references;
        }
    }
    ```

    Add after parse_args() function, before run_status().
  </action>
  <verify>cargo check succeeds</verify>
  <done>ExecutionTracker helper struct added to main.rs</done>
</task>

<task type="auto">
  <name>Task 3: Wrap run_status with execution tracking</name>
  <files>src/main.rs</files>
  <action>
    Update run_status() to use ExecutionTracker:

    Pattern:
    ```rust
    fn run_status(db_path: PathBuf, output_format: OutputFormat) -> Result<()> {
        let mut graph = CodeGraph::open(&db_path)?;
        let mut tracker = ExecutionTracker::new(
            vec!["status".to_string()],
            None,
            db_path.to_string_lossy().to_string(),
        );
        tracker.start(&graph)?;

        let file_count = graph.count_files()?;
        // ... rest of function ...

        tracker.finish(&graph)?;
        Ok(())
    }
    ```

    Also add exec_id from tracker to JSON response:
    ```rust
    let exec_id = tracker.exec_id.clone();
    let json_response = JsonResponse::new(response, &exec_id);
    ```
  </action>
  <verify>cargo check succeeds, run_status compiles</verify>
  <done>run_status uses ExecutionTracker for execution logging</done>
</task>

<task type="auto">
  <name>Task 4: Wrap run_files with execution tracking</name>
  <files>src/main.rs</files>
  <action>
    Update run_files() to use ExecutionTracker:
    1. Create tracker before graph operations
    2. Call tracker.start(&graph)?
    3. Set counts: tracker.set_counts(file_nodes.len(), 0, 0)
    4. Call tracker.finish(&graph)? before return
    5. Use tracker.exec_id for JSON response

    Follow same pattern as run_status.
  </action>
  <verify>cargo check succeeds, run_files compiles</verify>
  <done>run_files uses ExecutionTracker for execution logging</done>
</task>

<task type="auto">
  <name>Task 5: Wrap run_export with execution tracking</name>
  <files>src/main.rs</files>
  <action>
    Update run_export() to use ExecutionTracker:
    1. Create tracker with command args
    2. Call tracker.start(&graph)?
    3. Call tracker.finish(&graph)? before return
    4. Export doesn't use JSON response format (raw JSON output)
  </action>
  <verify>cargo check succeeds, run_export compiles</verify>
  <done>run_export uses ExecutionTracker for execution logging</done>
</task>

<task type="auto">
  <name>Task 6: Wrap run_label with execution tracking</name>
  <files>src/main.rs</files>
  <action>
    Update run_label() to use ExecutionTracker:
    1. Create tracker with command args including labels
    2. Call tracker.start(&graph)?
    3. Call tracker.finish(&graph)? before return
    4. run_label has human output, not JSON

    Handle list and count modes within same function.
  </action>
  <verify>cargo check succeeds, run_label compiles</verify>
  <done>run_label uses ExecutionTracker for execution logging</done>
</task>

<task type="auto">
  <name>Task 7: Update query_cmd.rs with execution tracking</name>
  <files>src/query_cmd.rs</files>
  <action>
    Read src/query_cmd.rs and add ExecutionTracker to run_query():
    1. Create ExecutionTracker at function start
    2. Call tracker.start(&graph)?
    3. Pass tracker.exec_id to JsonResponse::new()
    4. Call tracker.finish(&graph)? before return

    The query command returns JSON, so execution_id should be included.
  </action>
  <verify>cargo check succeeds, query_cmd compiles</verify>
  <done>run_query uses ExecutionTracker and includes execution_id in JSON</done>
</task>

<task type="auto">
  <name>Task 8: Update find_cmd.rs with execution tracking</name>
  <files>src/find_cmd.rs</files>
  <action>
    Read src/find_cmd.rs and add ExecutionTracker to run_find():
    1. Create ExecutionTracker at function start
    2. Call tracker.start(&graph)?
    3. Pass tracker.exec_id to JsonResponse::new()
    4. Call tracker.finish(&graph)? before return
  </action>
  <verify>cargo check succeeds, find_cmd compiles</verify>
  <done>run_find uses ExecutionTracker and includes execution_id in JSON</done>
</task>

<task type="auto">
  <name>Task 9: Update refs_cmd.rs with execution tracking</name>
  <files>src/refs_cmd.rs</files>
  <action>
    Read src/refs_cmd.rs and add ExecutionTracker to run_refs():
    1. Create ExecutionTracker at function start
    2. Call tracker.start(&graph)?
    3. Pass tracker.exec_id to JsonResponse::new()
    4. Call tracker.finish(&graph)? before return
  </action>
  <verify>cargo check succeeds, refs_cmd compiles</verify>
  <done>run_refs uses ExecutionTracker and includes execution_id in JSON</done>
</task>

<task type="auto">
  <name>Task 10: Update get_cmd.rs with execution tracking</name>
  <files>src/get_cmd.rs</files>
  <action>
    Read src/get_cmd.rs and add ExecutionTracker to run_get() and run_get_file():
    1. Create ExecutionTracker at function start for each
    2. Call tracker.start(&graph)?
    3. Call tracker.finish(&graph)? before return
    4. get commands have human output, not JSON
  </action>
  <verify>cargo check succeeds, get_cmd compiles</verify>
  <done>run_get and run_get_file use ExecutionTracker for execution logging</done>
</task>

<task type="auto">
  <name>Task 11: Update verify_cmd.rs with execution tracking</name>
  <files>src/verify_cmd.rs</files>
  <action>
    Read src/verify_cmd.rs and add ExecutionTracker to run_verify():
    1. Create ExecutionTracker at function start
    2. Call tracker.start(&graph)?
    3. Call tracker.finish(&graph)? before return
    4. Handle outcome based on verification results
  </action>
  <verify>cargo check succeeds, verify_cmd compiles</verify>
  <done>run_verify uses ExecutionTracker for execution logging</done>
</task>

<task type="auto">
  <name>Task 12: Update watch_cmd.rs with execution tracking</name>
  <files>src/watch_cmd.rs</files>
  <action>
    Read src/watch_cmd.rs and add ExecutionTracker to run_watch():
    1. Create ExecutionTracker at function start
    2. Call tracker.start(&graph)?
    3. Watch mode runs indefinitely - update outcome periodically or only at exit
    4. Call tracker.finish(&graph)? when watch exits

    For long-running watch, treat as single execution with outcome set at exit.
  </action>
  <verify>cargo check succeeds, watch_cmd compiles</verify>
  <done>run_watch uses ExecutionTracker for execution logging</done>
</task>

</tasks>

<verification>
1. cargo check --workspace passes without errors
2. All command handlers call ExecutionTracker::start and ::finish
3. JSON responses include execution_id from tracker
4. Errors set tracker.set_error() before finish
5. execution_log table populated after running commands
</verification>

<success_criteria>
1. Every CLI command generates unique execution_id
2. Every command execution recorded in execution_log table
3. JSON outputs include execution_id in JsonResponse wrapper
4. Errors recorded with error_message and outcome="error"
5. All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-stable-identity/05-04-SUMMARY.md`
</output>
